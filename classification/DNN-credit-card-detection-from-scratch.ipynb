{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Credit Card Detection From Scratch \n",
    "\n",
    "# NOT COMPLETE\n",
    "\n",
    "* The model performs poorly, either the dataset isn't processed correctly or no significant insights can be derived from the data or this DNN architecture isn't the right one, for temporal datasets LSTM are a better fit.\n",
    "\n",
    "[kaggle dataset url](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data)\n",
    "\n",
    "The model will \n",
    "* use Adam Optimizer\n",
    "* schedule the learning rate to decrease by epoch, also see how it performs when the learning rate stays the same\n",
    "* use mini-batches for the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From kaggle web dataset:\n",
    "> It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/creditcard.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 30 features, and the labels is either a 0 for no fraud or a 1 if that transaction was fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(df[\"Class\"].unique())\n",
    "num_classes # the number of classes model has to predict given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the labels and features\n",
    "Y_df = df[[\"Class\"]]\n",
    "X_df = df.drop(columns=[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Amount  \n",
       "0       149.62  \n",
       "1         2.69  \n",
       "2       378.66  \n",
       "3       123.50  \n",
       "4        69.99  \n",
       "...        ...  \n",
       "284802    0.77  \n",
       "284803   24.79  \n",
       "284804   67.88  \n",
       "284805   10.00  \n",
       "284806  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>6.406204e-16</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.709250e-01</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.449772e+01</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.117214e-01</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.248109e-02</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330408e-01</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.942090e+01</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V20           V21           V22           V23  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  6.406204e-16  1.654067e-16 -3.568593e-16  2.578648e-16   \n",
       "std    ...  7.709250e-01  7.345240e-01  7.257016e-01  6.244603e-01   \n",
       "min    ... -5.449772e+01 -3.483038e+01 -1.093314e+01 -4.480774e+01   \n",
       "25%    ... -2.117214e-01 -2.283949e-01 -5.423504e-01 -1.618463e-01   \n",
       "50%    ... -6.248109e-02 -2.945017e-02  6.781943e-03 -1.119293e-02   \n",
       "75%    ...  1.330408e-01  1.863772e-01  5.285536e-01  1.476421e-01   \n",
       "max    ...  3.942090e+01  2.720284e+01  1.050309e+01  2.252841e+01   \n",
       "\n",
       "                V24           V25           V26           V27           V28  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   4.473266e-15  5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16   \n",
       "std    6.056471e-01  5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01   \n",
       "min   -2.836627e+00 -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01   \n",
       "25%   -3.545861e-01 -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02   \n",
       "50%    4.097606e-02  1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02   \n",
       "75%    4.395266e-01  3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02   \n",
       "max    4.584549e+00  7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   \n",
       "\n",
       "              Amount  \n",
       "count  284807.000000  \n",
       "mean       88.349619  \n",
       "std       250.120109  \n",
       "min         0.000000  \n",
       "25%         5.600000  \n",
       "50%        22.000000  \n",
       "75%        77.165000  \n",
       "max     25691.160000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split And Normalize\n",
    "subtract the mean and divide by the standard deviation of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_data(X_df, Y_df):\n",
    "    # split the df into train val and test sets with tuples of (features, labels)\n",
    "    # 70% 20% 10%\n",
    "    # column_indices = {name: i for i, name in enumerate(df.drop.columns)}\n",
    "    # normalize, You should save the mean and std values to use during inference\n",
    "    train_mean = X_df.mean()\n",
    "    train_std = X_df.std()\n",
    "\n",
    "    n = len(X_df)\n",
    "    X_train = X_df[0:int(n*0.7)]\n",
    "    Y_train = Y_df[0:int(n*0.7)]\n",
    "\n",
    "    X_val = X_df[int(n*0.7):int(n*0.9)]\n",
    "    Y_val = Y_df[int(n*0.7):int(n*0.9)]\n",
    "\n",
    "    X_test = X_df[int(n*0.9):]\n",
    "    Y_test = X_df[int(n*0.9):]\n",
    "    # num_features = X_df.shape[1]\n",
    "\n",
    "    # now using the mean and std that was computed over the end features dataset use it to normalize the features\n",
    "    X_train = (X_train - train_mean) / train_std\n",
    "    X_val = (X_val - train_mean) / train_std\n",
    "    X_test = (X_test - train_mean) / train_std\n",
    "\n",
    "    test_set = (X_test.values, Y_test.values)\n",
    "    val_set = (X_val.values, Y_val.values)\n",
    "    train_set = (X_train.values, Y_train.values)\n",
    "\n",
    "    return train_set, val_set, test_set, train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Amount  \n",
       "0       149.62  \n",
       "1         2.69  \n",
       "2       378.66  \n",
       "3       123.50  \n",
       "4        69.99  \n",
       "...        ...  \n",
       "284802    0.77  \n",
       "284803   24.79  \n",
       "284804   67.88  \n",
       "284805   10.00  \n",
       "284806  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "\n",
       "         V8        V9  ...       V20       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head(1) # before normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set, train_mean, train_std = Split_data(X_df, Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.99657952, -0.6942411 , -0.04407485,  1.67277056,  0.97336381,\n",
       "        -0.24511615,  0.34706734,  0.1936786 ,  0.08263713,  0.3311272 ,\n",
       "         0.0833854 , -0.54040609, -0.61829463, -0.99609717, -0.32460962,\n",
       "         1.60401102, -0.53683193,  0.24486302,  0.03076988,  0.49628116,\n",
       "         0.32611744, -0.02492332,  0.38285377, -0.17691102,  0.11050673,\n",
       "         0.24658501, -0.39216974,  0.33089104, -0.06378104,  0.24496383]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][:1, :] # after normalization now its a numpy array instead of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199364, 30), (199364, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape, train_set[1].shape\n",
    "# (num_examples,   num_features_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, peek at the distribution of the features. Some features do have long tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/7xftw3_12y10ppg5bv5lb26w0000gn/T/ipykernel_85654/3214313372.py:5: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  _ = ax.set_xticklabels(df.keys(), rotation=90)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_std = (df - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Features and labels Into Batches\n",
    "\n",
    "With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.\n",
    "\n",
    "* Also im shuffling the data, normally for timeseries data u want to keep the data unshuffled but I dont think for transactional data it matters much. And we only shuffle the train features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, input size)\n",
    "    Y -- true \"label\" vector (1 for fraud / 0 for no fraud), of shape (number of examples, 1)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0] # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :].reshape((m, 1))\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Now we partition the examples into mini-bactches, this is only for the group of examples that fit into 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of complete mini batches that can be made their will be some examples left over\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[ inc * k : inc * (k + 1), : ]\n",
    "        mini_batch_Y = shuffled_Y[ inc * k : inc * (k + 1), : ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case for the last examples that are less than the mini-batch size, example we have 40 examples left\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[ -(m - mini_batch_size * num_complete_minibatches) :,  ]\n",
    "        mini_batch_Y = shuffled_Y[ -(m - mini_batch_size * num_complete_minibatches) :,  ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example of above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3115, 3115.0625)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of what the funtion does above\n",
    "m = train_set[0].shape[0] # number of training examples\n",
    "mini_batches = []\n",
    "\n",
    "# Step 1: Shuffle (X, Y)\n",
    "permutation = list(np.random.permutation(m))\n",
    "shuffled_X = train_set[0][permutation, :]\n",
    "shuffled_Y = train_set[1][permutation, :].reshape((m, 1))\n",
    "mini_batch_size = 64\n",
    "inc = mini_batch_size\n",
    "\n",
    "# Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "# Now we partition the examples into mini-bactches, this is only for the group of examples that fit into 64 examples.\n",
    "num_complete_minibatches = math.floor(m / mini_batch_size) # number of complete mini batches that can be made their will be some examples left over\n",
    "num_complete_minibatches, train_set[0].shape[0] / mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, num_complete_minibatches):\n",
    "    # print(inc) # = 64\n",
    "    # print(k) # loop through each mini batch that can be made\n",
    "    # print(inc * k) # gives num from 0 to 64 to 128 etcc size of minibatches\n",
    "    # print(inc * k)\n",
    "    # below slices the examples into a batch the inc * k is the start of a slice, the the inc * (k + 1) is the end of 0 to 64, 64 to 128 etc..\n",
    "    mini_batch_X = shuffled_X[ inc * k : inc * (k + 1), : ]\n",
    "    mini_batch_Y = shuffled_Y[ inc * k : inc * (k + 1), : ]\n",
    "    \n",
    "    mini_batch = (mini_batch_X, mini_batch_Y) # append to arr of tuple with corresponding (feature, label)\n",
    "    mini_batches.append(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = mini_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3115.0625"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "199364 / mini_batch_size # .0625 examples left over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364, 30)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 30)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_X[ -(m - mini_batch_size * num_complete_minibatches) :,  ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_Y[ -(m - mini_batch_size * num_complete_minibatches) :,  ].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimizer\n",
    "\n",
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum. \n",
    "\n",
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = np.zeros(( parameters[\"W\" + str(l)].shape ))\n",
    "        v[\"db\" + str(l)] = np.zeros(( parameters[\"b\" + str(l)].shape ))\n",
    "        s[\"dW\" + str(l)] = np.zeros(( parameters[\"W\" + str(l)].shape ))\n",
    "        s[\"db\" + str(l)] = np.zeros(( parameters[\"b\" + str(l)].shape ))\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- Adam variable, counts the number of taken steps\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + ( 1 - beta1 ) * grads[\"dW\" + str(l)]\n",
    "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + ( 1 - beta1 ) * grads[\"db\" + str(l)]\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - ( beta1 )**t )\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - ( beta1 )**t )\n",
    "        \n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + ( 1 - beta2 ) * (grads[\"dW\" + str(l)])**2\n",
    "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + ( 1 - beta2 ) * (grads[\"db\" + str(l)])**2\n",
    "        \n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / ( 1 - (beta2)**t ) \n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / ( 1 - (beta2)**t )         \n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * ( v_corrected[\"dW\" + str(l)] / ( np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon) )\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * ( v_corrected[\"db\" + str(l)] / ( np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon) ) \n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    # the first layer is the input layer which has the examples per example we dont have Weights and biases for that layer just the hidden layers\n",
    "    for l in range(1, L):\n",
    "        # so the weight matrix is the neurons at that layer and then taking in the neurons from the previous layer,\n",
    "        # in the first hidden layer, it has example 10 neurons then it takes in the input data of example 20 features so that \n",
    "        # layer will have a shape of (10, 20), same with bias but just 10 neuron, by 1\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def compute_cost(a3, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the cost function, this one is Binary Cross-Entropy Loss\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, same shape as a3\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the cost function without dividing by number of training examples\n",
    "    \n",
    "    Note: \n",
    "    This is used with mini-batches, so we'll first accumulate costs over an entire epoch and then divide by the m training examples\n",
    "    this loss function is commonly used to measure the difference between the predicted probabilities (a3 in this case) and the actual labels (Y).\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-5 # we use a small epsilon value so that the function below doesn't divide by zero if chance\n",
    "    log_probs = np.multiply(-np.log(a3 + epsilon), Y) + np.multiply(-np.log(1 - a3 + epsilon), 1 - Y)\n",
    "    cost_total = np.sum(log_probs)\n",
    "    # without epsilon\n",
    "    # log_probs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    # cost_total =  np.sum(log_probs)\n",
    "    \n",
    "    return cost_total\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss).\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (number of examples, input size)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    # print(\"W1 shape:\", W1.shape)\n",
    "    # print(\"X shape:\", X.T.shape,\"x shape normal\", X.shape)\n",
    "    z1 = np.dot(W1, X.T) + b1\n",
    "    # print(\"z1 shape:\", z1.shape)\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (number of examples, input size)\n",
    "    Y -- true \"label\" vector (containing 0 if non-fraud, 1 if fraud)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = 1./m * (a3 - Y.T)\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    p = np.zeros((m, 1), dtype = int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    # return a3, p\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[i, 0] = 1\n",
    "        else:\n",
    "            p[i, 0] = 0\n",
    "\n",
    "    # print (\"predictions: \" + str(p[0,:]))\n",
    "    # print (\"true labels: \" + str(y[0,:]))\n",
    "    # print(\"Accuracy: \"  + str(np.mean((p[:, 0] == y[:, 0]))))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "        beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 50, print_cost = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, input)\n",
    "    Y -- true \"label\" vector (1 for fraud / 0 no fraud), of shape (number of examples, 1)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs the number of times the model sees the whole dataset, if 50 the model sees the dataset 50 times\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    m = X.shape[0]                   # number of training examples\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    v, s = initialize_adam(parameters)\n",
    "    # seed = 1\n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        # seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "            # return a3, caches, minibatch_X, minibatch_Y #testing\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        # Print the cost every 5 epochs\n",
    "        if print_cost and i % 5 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 5 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 5)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TESTING MODEL Ignore, code only to clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_size = train_set[0].shape[1] # the amount of features per example\n",
    "layers_dims = [input_layer_size, 5, 2, 1]\n",
    "input_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = train_set\n",
    "a3, caches, minibatch_X, minibatch_Y  = model(train_X, train_Y, layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3.shape #the output of the last minibatch is 1, this is only one loop so we only got one minibatch with 64 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 64),\n",
       " (5, 64),\n",
       " (5, 30),\n",
       " (5, 1),\n",
       " (2, 64),\n",
       " (2, 64),\n",
       " (2, 5),\n",
       " (2, 1),\n",
       " (1, 64),\n",
       " (1, 64),\n",
       " (1, 2),\n",
       " (1, 1))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3 = caches\n",
    "z1.shape, a1.shape, W1.shape, b1.shape, z2.shape, a2.shape, W2.shape, b2.shape, z3.shape, a3.shape, W3.shape, b3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 6.714544\n",
      "Cost after epoch 5: 1.284390\n",
      "Cost after epoch 10: 1.348642\n",
      "Cost after epoch 15: 1.396907\n",
      "Cost after epoch 20: 1.418634\n",
      "Cost after epoch 25: 1.425964\n",
      "Cost after epoch 30: 1.421834\n",
      "Cost after epoch 35: 1.433442\n",
      "Cost after epoch 40: 1.450739\n",
      "Cost after epoch 45: 1.456931\n",
      "Cost after epoch 50: 1.456913\n",
      "Cost after epoch 55: 1.460704\n",
      "Cost after epoch 60: 1.483865\n",
      "Cost after epoch 65: 1.473617\n",
      "Cost after epoch 70: 1.472461\n",
      "Cost after epoch 75: 1.472337\n",
      "Cost after epoch 80: 1.488360\n",
      "Cost after epoch 85: 1.487793\n",
      "Cost after epoch 90: 1.491437\n",
      "Cost after epoch 95: 1.491852\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train 3-layer model, # for more layers you would need to edit the model and parameters its hardcoded for 3 layers\n",
    "input_layer_size = train_set[0].shape[1] # the amount of features per example\n",
    "mini_batch_size = 64\n",
    "layers_dims = [input_layer_size, 5, 2, 1]\n",
    "train_X, train_Y = train_set\n",
    "parameters = model(train_X, train_Y, layers_dims, mini_batch_size=mini_batch_size, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.12156351e+00, -9.23778333e-01, -2.13316638e-01,\n",
       "         -2.01607791e-02, -1.68762169e-01, -1.08993666e+00,\n",
       "          2.02264246e-01, -3.29563682e-01,  4.52458357e-01,\n",
       "          3.50651935e-01, -1.23929665e-01,  7.94477811e-01,\n",
       "          3.79148255e-01,  5.49190071e-01,  3.63354231e-01,\n",
       "          5.23129994e-02,  1.06547964e-01, -8.54839181e-02,\n",
       "          1.38105501e+00, -2.10051485e-01, -9.40049020e-01,\n",
       "         -5.35050419e-01, -4.65180958e-01,  3.66049510e-01,\n",
       "         -1.71084691e+00, -1.35222850e+00,  2.77846422e-01,\n",
       "         -3.05251201e-01,  1.52477075e-01, -2.96582041e-02],\n",
       "        [ 3.77846703e-01,  3.47014459e-01,  5.72630636e-02,\n",
       "         -1.63584944e-01, -6.72832398e-01,  9.77505278e-01,\n",
       "         -2.73775243e-01,  2.63007023e-01, -2.18814446e-01,\n",
       "          1.09335676e-01,  3.41800895e-01, -1.07661779e+00,\n",
       "         -3.02047182e-01, -5.79401569e-01, -4.18383130e-02,\n",
       "         -3.94477628e-01, -3.00333178e-01,  6.31226921e-02,\n",
       "         -3.56172762e-01, -6.11895421e-01,  9.78388642e-01,\n",
       "         -2.03589231e-01, -1.25027218e-01,  3.94504981e-01,\n",
       "          4.11235690e-01, -1.38681008e-01,  2.07439096e-01,\n",
       "         -1.92452344e-02, -3.83741240e-01, -8.92076342e-01],\n",
       "        [-4.40436079e-02,  2.91085298e-01,  3.76649460e-01,\n",
       "          1.90322631e-01,  3.91759216e-01, -6.21143553e-02,\n",
       "         -1.74754085e-01, -6.40241147e-01,  1.49115953e-01,\n",
       "         -7.99593222e-02, -4.14605805e-01,  4.45133845e-02,\n",
       "         -4.21352176e-01, -2.26517018e-01, -4.12690313e-01,\n",
       "         -4.08465547e-01, -6.20611405e-01, -1.78068343e-01,\n",
       "         -1.17643826e-03, -8.88289179e-01, -3.38808980e-01,\n",
       "          4.82612255e-02, -5.10476955e-01, -5.23535976e-01,\n",
       "          1.81331055e-01, -4.28984013e-01, -2.07869842e-01,\n",
       "          9.35059345e-02, -7.93955120e-02, -1.56824528e-02],\n",
       "        [-8.74587238e-02,  6.05694420e-02, -4.87369118e-01,\n",
       "          1.37916495e+00, -1.44257616e-01, -5.21881144e-01,\n",
       "          9.39165023e-04, -6.64586394e-01,  5.68782169e-01,\n",
       "         -7.81421952e-01,  4.65767267e-01, -1.56791557e-01,\n",
       "          3.29415914e-01, -9.99223799e-02,  3.53809663e-01,\n",
       "         -7.37563115e-01,  4.18051988e-01,  7.04653656e-03,\n",
       "         -7.80089402e-01,  4.40698349e-01,  3.95215436e-02,\n",
       "          6.84492796e-01, -6.02817261e-01, -1.66479413e-02,\n",
       "          1.15149922e-01,  6.84993033e-01, -7.91878717e-03,\n",
       "         -9.79032808e-02,  4.87317767e-02,  6.29801712e-01],\n",
       "        [-1.09094800e+00,  1.61306603e-01,  9.25881593e-01,\n",
       "         -1.69455925e-01, -2.40629210e-01,  3.36835269e-01,\n",
       "          4.20000487e-01,  1.94428565e-02,  4.55229279e-01,\n",
       "          4.09072812e-01,  1.56009814e-01,  2.93867732e-01,\n",
       "          7.21585213e-01,  7.81093853e-01,  9.51839694e-01,\n",
       "          8.12300851e-01,  2.10223383e-01,  6.32580243e-01,\n",
       "          2.12414472e-01,  3.13386670e-02, -2.17085153e-01,\n",
       "          2.06207501e-01,  5.36781472e-01, -9.38916788e-01,\n",
       "          1.02423737e+00,  5.69130401e-01, -2.45034092e-01,\n",
       "          3.92092578e-01, -2.91911066e-01,  2.51621408e-01]]),\n",
       " 'b1': array([[ 2.07814537],\n",
       "        [ 2.05317868],\n",
       "        [-0.96526367],\n",
       "        [ 1.15439079],\n",
       "        [ 1.60115864]]),\n",
       " 'W2': array([[ 0.00496746, -0.82453983, -0.12728558, -0.41462471,  0.00739607],\n",
       "        [ 1.12061267,  1.75216527, -1.43417719,  1.40218529,  1.36584877]]),\n",
       " 'b2': array([[-0.12031844],\n",
       "        [ 1.19960507]]),\n",
       " 'W3': array([[ 1.83132139, -1.03244922]]),\n",
       " 'b3': array([[1.67341847]])}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 30)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, Y_val = val_set\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 1)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(X_val, Y_val, parameters)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 0\n"
     ]
    }
   ],
   "source": [
    "# it would be easy for the model to predict zero as its unsure so, test only if it \n",
    "# correctly predicts a fraud transactions\n",
    "# Create a boolean mask where both pred and Y have a value of 1\n",
    "mask = (predictions == 1) & (Y == 1)\n",
    "# Count the number of True values in the mask\n",
    "num_matches = mask.sum()\n",
    "# Print the count of matches\n",
    "print(\"Number of matches:\", num_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
